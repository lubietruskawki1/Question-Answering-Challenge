{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Wikipedia dump for offline retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in https://en.wikipedia.org/wiki/Wikipedia:Database_download, we download the Polish Wikipedia dump from https://dumps.wikimedia.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://dumps.wikimedia.org/plwiki/latest/plwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    file_name = 'plwiki-latest-pages-articles.xml.bz2'\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = 1024  # 1 KB\n",
    "    with open(file_name, 'wb') as file, tqdm(\n",
    "        desc=file_name,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    print(f'Wikipedia dump downloaded successfully: {file_name}')\n",
    "else:\n",
    "    print(f\"Failed to download the Wikipedia dump. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from the Wikipedia dump using WikiExtractor\n",
    "\n",
    "Run the following command to extract articles from the dump into an `extracted/` folder:\n",
    "```bash\n",
    "wikiextractor --json plwiki-latest-pages-articles.xml.bz2 -o extracted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the extracted data and indexing it into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szkar\\AppData\\Local\\Temp\\ipykernel_25284\\2279654839.py:6: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.indices.delete(index=INDEX_NAME, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "INDEX_BODY = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"paragraph_number\": {\"type\": \"integer\"},\n",
    "            \"content\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# UTWORZENIE INDEKSU (jeśli nie istnieje)\n",
    "if not es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_BODY)\n",
    "    # Wyłączamy odświeżanie i replikę na czas indeksowania (przyspieszy to import)\n",
    "    es.indices.put_settings(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"index\": {\n",
    "                \"refresh_interval\": \"-1\",\n",
    "                \"number_of_replicas\": 0\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created index '{INDEX_NAME}' with temporary settings (no refresh, 0 replicas).\")\n",
    "\n",
    "# DEFINICJA GENERATORA DOKUMENTÓW\n",
    "def generate_actions(dump_dir, index_name):\n",
    "    \"\"\"\n",
    "    Generator dla parallel_bulk. Dla każdego pliku wiki_... wyciąga artykuły,\n",
    "    dzieli je na paragrafy i yielduje dokumenty do zindeksowania.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(dump_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('wiki_'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        article = json.loads(line)\n",
    "                        title = article.get('title', '')\n",
    "                        content = article.get('text', '')\n",
    "\n",
    "                        paragraphs = content.split(\"\\n\")\n",
    "                        for i, paragraph in enumerate(paragraphs):\n",
    "                            paragraph = paragraph.strip()\n",
    "                            if paragraph:\n",
    "                                yield {\n",
    "                                    \"_index\": index_name,\n",
    "                                    \"_source\": {\n",
    "                                        \"title\": title,\n",
    "                                        \"paragraph_number\": i,\n",
    "                                        \"content\": paragraph\n",
    "                                    }\n",
    "                                }\n",
    "\n",
    "# WYWOŁANIE parallel_bulk DO MASOWEGO INDEKSOWANIA\n",
    "actions = generate_actions(\"extracted\", INDEX_NAME)\n",
    "\n",
    "successes = 0\n",
    "for ok, resp in parallel_bulk(\n",
    "    client=es,\n",
    "    actions=actions,\n",
    "    thread_count=4,      # liczba wątków\n",
    "    chunk_size=500,      # wielkość jednej paczki dokumentów\n",
    "    max_chunk_bytes=10 * 1024 * 1024  # ~10 MB na paczkę\n",
    "):\n",
    "    if not ok:\n",
    "        logging.error(f\"Error indexing chunk: {resp}\")\n",
    "    else:\n",
    "        successes += 1\n",
    "\n",
    "print(f\"Successfully processed {successes} chunks of data.\")\n",
    "\n",
    "# PRZYWRACANIE NORMALNYCH USTAWIEŃ\n",
    "es.indices.put_settings(\n",
    "    index=INDEX_NAME,\n",
    "    body={\n",
    "        \"index\": {\n",
    "            \"refresh_interval\": \"1s\",\n",
    "            \"number_of_replicas\": 1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Indexing complete. Restored normal settings for '{INDEX_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve keywords from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "def get_keywords(question):\n",
    "    doc = nlp(question)\n",
    "    keywords = [token.text for token in doc if not token.is_stop] # and token.pos_ in [\"NOUN\", \"VERB\", \"NUM\", \"PROPN\"]\n",
    "    return \" \".join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and answer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at henryk/bert-base-multilingual-cased-finetuned-polish-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Inicjalizacja Elasticsearch i pipeline QA\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "#qa_pipeline = pipeline(\"question-answering\", model=\"radlab/polish-qa-v2\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"henryk/bert-base-multilingual-cased-finetuned-polish-squad2\", device=0)\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=\"sdadas/polish-roberta-large-v2\") <- super bad\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=\"sdadas/polish-gpt2-xl\") <- super bad as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3.1\", server_url=\"http://localhost:11434\"):\n",
    "    url = f\"{server_url}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Przetwarzanie strumienia odpowiedzi\n",
    "        result = \"\"\n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                data = json.loads(chunk.decode('utf-8'))\n",
    "                if \"response\" in data:\n",
    "                    result += data[\"response\"]\n",
    "                if data.get(\"done\"):\n",
    "                    break\n",
    "        return result\n",
    "    else:\n",
    "        raise Exception(f\"Błąd: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model odpowiedział: LLaMA 3 jest wielowarstwowym modelem języka, który może wykonać takie operacje jak generowanie tekstu, interpretacja języka naturalnego i odpowiedzi na pytania.\n"
     ]
    }
   ],
   "source": [
    "# Przykład użycia:\n",
    "prompt = \"Opisz funkcjonalność modelu LLama 3.1 w jednym zdaniu.\"\n",
    "try:\n",
    "    answer = query_ollama(prompt)\n",
    "    print(\"Model odpowiedział:\", answer)\n",
    "except Exception as e:\n",
    "    print(\"Wystąpił błąd:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "def extract_symbols(text):\n",
    "    pattern = r'\\b[A-Z][a-zA-Z]?\\b'\n",
    "    symbols = re.findall(pattern, text)\n",
    "    return symbols\n",
    "\n",
    "def is_important_token(token, text, symbols_set):\n",
    "    return (token.pos_ in [\"NOUN\", \"VERB\", \"NUM\", \"ADJ\", \"ADV\", \"DET\"]\n",
    "            and token.text not in symbols_set\n",
    "            and not token.is_stop)\n",
    "\n",
    "def get_keywords(question):\n",
    "    doc = nlp(question)\n",
    "    all_symbols = extract_symbols(question)\n",
    "    symbols_set = {sym for sym in all_symbols if not nlp(sym)[0].is_stop}\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in doc\n",
    "        if is_important_token(token, question, symbols_set)\n",
    "    ]\n",
    "    return list(symbols_set), list(set(keywords))\n",
    "\n",
    "def extractentities(question):\n",
    "    doc = nlp(question)\n",
    "    entities = [(ent.text, ent.lemma_, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def extract_and_remove_quotes(text):\n",
    "    pattern = r\"'(.*?)'\"\n",
    "    quotes = re.findall(pattern, text)\n",
    "    modified_text = re.sub(pattern, '', text)\n",
    "    return quotes, modified_text\n",
    "\n",
    "def ultimate_parse(question):\n",
    "    entities = extractentities(question)\n",
    "    text = question\n",
    "    for ent_text in [t for (t, _, _) in entities]:\n",
    "        text = text.replace(ent_text, \"\")\n",
    "    quotes, modified_text = extract_and_remove_quotes(text)\n",
    "    lema_entities = [l for (_, l, _) in entities]\n",
    "    symbols, tokens = get_keywords(modified_text)\n",
    "\n",
    "    return quotes, lema_entities, symbols, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['Bombaj'], [], ['znajdować', 'państwo', 'jaki'])\n",
      "([], [], ['Bq'], ['symbol', 'nazywać', 'pierwiastek'])\n",
      "([], [], [], ['pełny', 'rok', 'kalendarzowy', 'tydzień'])\n",
      "(['W pustyni i w puszczy'], [], [], ['państwo', 'akcja', 'powieść', 'rozpoczynać'])\n",
      "([], ['XIX wiek'], [], ['sztywny', 'modny', 'rondelke', 'wąski', 'zaokrąglony', 'unieść', 'kapelusz', 'drugi', 'Męski', 'nazywać', 'lekko', 'główka', 'połowa'])\n",
      "([], [], [], ['ciecz', 'wklęsła', 'pobliże', 'ścianek', 'wypuknąć', 'nazywać', 'naczynie', 'powierzchnia'])\n",
      "([], ['Horeszk Gerwaz'], [], ['ulubiony', 'brzmieć', 'klucznika', 'powiedzonko'])\n",
      "([], [], [], ['powietrze', 'najwięcej', 'gaz'])\n"
     ]
    }
   ],
   "source": [
    "print(ultimate_parse(\"W jakim państwie znajduje się Bombaj\"))\n",
    "print(ultimate_parse(\"Jak nazywa się pierwiastek o symbolu Bq\"))\n",
    "print(ultimate_parse(\"Ile pełnych tygodni ma rok kalendarzowy\"))\n",
    "print(ultimate_parse(\"W którym państwie rozpoczyna się akcja powieści 'W pustyni i w puszczy'?\"))\n",
    "print(ultimate_parse(\"Jak nazywał się sztywny kapelusz męski o zaokrąglonej główce i wąskim lekko uniesionym rondelku, modny w drugiej połowie XIX wieku\"))\n",
    "print(ultimate_parse(\"Jak nazywa się wypukła albo wklęsła powierzchnia cieczy w pobliżu ścianek naczynia\"))\n",
    "print(ultimate_parse(\"Jak brzmi ulubione powiedzonko klucznika Horeszków Gerwazego?\"))\n",
    "question = \"Którego gazu jest najwięcej w powietrzu?\"#\"Kto stworzył Tomb Raider?\"\n",
    "print(ultimate_parse(question))\n",
    "# print(get_keywords(question))\n",
    "# answer_question(question, es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "# 2. Funkcja pobierająca akapity z ES\n",
    "def retrieve_paragraphs(question, es_client, index_name, size=10):\n",
    "    quotes, lema_entities, symbols, tokens = ultimate_parse(question)\n",
    "\n",
    "    must_clauses = []\n",
    "    for quote in quotes:\n",
    "        must_clauses.append({\"match_phrase\": {\"content\": quote}})\n",
    "\n",
    "    should_clauses = []\n",
    "    if lema_entities:\n",
    "        should_clauses.append({\n",
    "            \"multi_match\": {\n",
    "                \"query\": \" \".join(lema_entities),\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"fuzziness\": \"AUTO\",\n",
    "                \"boost\": 2.0\n",
    "            }\n",
    "        })\n",
    "    if symbols:\n",
    "        should_clauses.append({\n",
    "            \"multi_match\": {\n",
    "                \"query\": \" \".join(symbols),\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"fuzziness\": \"AUTO\",\n",
    "                \"boost\": 1.5\n",
    "            }\n",
    "        })\n",
    "    if tokens:\n",
    "        should_clauses.append({\n",
    "            \"multi_match\": {\n",
    "                \"query\": \" \".join(tokens),\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"fuzziness\": \"AUTO\",\n",
    "                \"boost\": 1.0\n",
    "            }\n",
    "        })\n",
    "\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": must_clauses,\n",
    "                \"should\": should_clauses,\n",
    "                \"minimum_should_match\": \"40%\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=query)\n",
    "\n",
    "    paragraphs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        paragraphs.append(hit[\"_source\"][\"content\"])\n",
    "    return paragraphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_answer_ollama(question, paragraphs):\n",
    "    # Zapytanie do modelu LLama\n",
    "    prompt = \"\"\n",
    "\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        prompt += f\"\\n\\n{i+1}: {paragraph}\"\n",
    "    try:        \n",
    "        prompt += \"Powyżej znajduje się kilka możliwych kontekstów które mogą pomóc udzielić odpowiedzi na pytanie. \"\n",
    "        prompt += \"Każdy w osobnym akapicie. Na ich podstawie proszę odpowiedzieć na pytanie. \"\n",
    "        prompt += \"Uwaga, nie wszystkie konteksty pasują do zadanego pytania. \"\n",
    "        prompt += \"Jeśli żaden kontekst nie pasuje, sam podaj odpowiedź. \"\n",
    "        prompt += \"Jeśli to możliwe - odpowiedz jednym słowem, jeśli nie, bardzo krótko, w mniej niż 5 słowach. \"\n",
    "        prompt += \"Nie wyjasniaj odpowiedzi ani sam nie dodawaj żadnych dodatkowych informacji. \"\n",
    "        prompt += \"Nie pisz również którego akapitu użyłeś. Chcę tylko odpowiedź na pytanie.\"\n",
    "        prompt += f\"\\n\\nPytanie: {question}\"\n",
    "        if question.lower().startswith(\"czy\"):\n",
    "            question += \" Odpowiedz koniecznie jednym słowem: tak/nie.\"\n",
    "        answer = query_ollama(prompt)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(\"Wystąpił błąd:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model odpowiedział: J.R.R. Tolkien\n"
     ]
    }
   ],
   "source": [
    "# test:\n",
    "question = \"Kto napisał Trylogię?\"\n",
    "paragraphs = [\"Trylogia to cykl powieści fantasy autorstwa J.R.R. Tolkiena.\", \"Trylogia to cykl powieści autorstwa Jana Kowalskiego.\"]\n",
    "\n",
    "answer = get_best_answer_ollama(question, paragraphs)\n",
    "print(\"Model odpowiedział:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Funkcja, która spośród pobranych paragrafów wybiera najlepszą odpowiedź\n",
    "def get_best_answer(question, paragraphs):\n",
    "    \"\"\"\n",
    "    Wywołuje pipeline QA dla każdego paragrafu i zwraca najlepszą odpowiedź wraz z wynikiem.\n",
    "    \"\"\"\n",
    "    def update_best(result, current_best):\n",
    "        \"\"\"Aktualizuje najlepszy wynik i odpowiedź.\"\"\"\n",
    "        if result[\"score\"] > current_best[\"score\"]:\n",
    "            return {\"answer\": result[\"answer\"], \"score\": result[\"score\"], \"context\": result[\"context\"]}\n",
    "        return current_best\n",
    "\n",
    "    # Inicjalizacja najlepszych wyników dla pytania i jego słów kluczowych\n",
    "    best_result = {\"answer\": None, \"score\": float(\"-inf\"), \"context\": None}\n",
    "    #best_result_kw = {\"answer\": None, \"score\": float(\"-inf\"), \"context\": None}\n",
    "\n",
    "    for paragraph in filter(str.strip, paragraphs):\n",
    "        result = qa_pipeline(question=question, context=paragraph)\n",
    "        result[\"context\"] = paragraph\n",
    "        best_result = update_best(result, best_result)\n",
    "\n",
    "        #kw_result = qa_pipeline(question=get_keywords(question), context=paragraph)\n",
    "        #kw_result[\"context\"] = paragraph\n",
    "        #best_result_kw = update_best(kw_result, best_result_kw)\n",
    "\n",
    "    # if best_result[\"context\"] and best_result_kw[\"context\"]:\n",
    "    #     combined_context = best_result[\"context\"] + best_result_kw[\"context\"]\n",
    "    #     combined_result = qa_pipeline(question=question, context=combined_context)\n",
    "    #     combined_result[\"context\"] = combined_context\n",
    "    #     best_result = update_best(combined_result, best_result)\n",
    "\n",
    "    # elif best_result_kw[\"context\"]:\n",
    "    #     best_result = best_result_kw\n",
    "\n",
    "    return best_result[\"answer\"], best_result[\"score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Przykładowe wywołanie\n",
    "\n",
    "def answer_question(question, es, index_name):\n",
    "    paragraphs = retrieve_paragraphs(question, es, index_name, size=8)\n",
    "    print(f\"Znaleziono {len(paragraphs)} pasujących paragrafów.\")\n",
    "    best_answer, best_score = get_best_answer(question, paragraphs)\n",
    "    ollama_answer = get_best_answer_ollama(question, paragraphs)\n",
    "    \n",
    "    print(f\"Pytanie: {question}\")\n",
    "    print(f\"Najlepsza odpowiedź: {best_answer}\")\n",
    "    print(f\"Score: {best_score}\")\n",
    "    print(f\"Odpowiedź z LLamy: {ollama_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono 8 pasujących paragrafów.\n",
      "Pytanie: Którego gazu jest najwięcej w powietrzu?\n",
      "Najlepsza odpowiedź: Wentylator\n",
      "Score: 0.6189001202583313\n",
      "Odpowiedź z LLamy: Powietrze.\n"
     ]
    }
   ],
   "source": [
    "question = \"Którego gazu jest najwięcej w powietrzu?\"#\"Kto stworzył Tomb Raider?\"\n",
    "# print(get_keywords(question))\n",
    "answer_question(question, es, INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono 8 pasujących paragrafów.\n",
      "Pytanie: Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\n",
      "Najlepsza odpowiedź: Lary Croft\n",
      "Score: 0.8891009092330933\n",
      "Odpowiedź z LLamy: Lara Croft.\n",
      "Pytanie: Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\n",
      "Najlepsza odpowiedź: 1945 na szczeblu poselstw\n",
      "Score: 0.04706767573952675\n",
      "Odpowiedź z LLamy: Tak.\n"
     ]
    }
   ],
   "source": [
    "question = \"Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\"\n",
    "answer_question(question, es, INDEX_NAME)\n",
    "\n",
    "question = \"Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\"\n",
    "paragraphs = retrieve_paragraphs(question, es, INDEX_NAME, size=10)\n",
    "question_tuned = \"Czy w starożytności powoływani byli posłowie i poselstwa? Tak czy nie?\"\n",
    "best_answer, best_score = get_best_answer(question_tuned, paragraphs)\n",
    "ollama_answer = get_best_answer_ollama(question_tuned, paragraphs)\n",
    "\n",
    "print(f\"Pytanie: {question}\")\n",
    "print(f\"Najlepsza odpowiedź: {best_answer}\")\n",
    "print(f\"Score: {best_score}\")\n",
    "print(f\"Odpowiedź z LLamy: {ollama_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono 8 pasujących paragrafów.\n",
      "Pytanie: Ile pełnych tygodni ma rok kalendarzowy?\n",
      "Najlepsza odpowiedź: 354, 384 lub 385\n",
      "Score: 0.21303628385066986\n",
      "Odpowiedź z LLamy: 52\n",
      "-------------------\n",
      "Znaleziono 8 pasujących paragrafów.\n",
      "Pytanie: Pierwiastek symbolu Bq?\n",
      "Najlepsza odpowiedź: bekerel\n",
      "Score: 0.5004624128341675\n",
      "Odpowiedź z LLamy: Bekerel.\n"
     ]
    }
   ],
   "source": [
    "question = \"Ile pełnych tygodni ma rok kalendarzowy?\"\n",
    "answer_question(question, es, INDEX_NAME)\n",
    "print('-------------------')\n",
    "\n",
    "question = \"Pierwiastek symbolu Bq?\"\n",
    "answer_question(question, es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 20\n",
      "Q: Jak nazywa się pierwsza litera alfabetu greckiego?\n",
      "A: Alfa.\n",
      "Expected: alfa\n",
      "Correct!\n",
      "\n",
      "Q: Jak nazywa się dowolny odcinek łączący dwa punkty okręgu?\n",
      "A: Cięciwa.\n",
      "Expected: cięciwa\n",
      "Correct!\n",
      "\n",
      "Q: W którym państwie rozpoczyna się akcja powieści „W pustyni i w puszczy”?\n",
      "A: Egipt.\n",
      "Expected: w Egipcie\n",
      "Correct!\n",
      "\n",
      "Q: Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\n",
      "A: Tak, historycznie.\n",
      "Expected: tak\n",
      "Incorrect!\n",
      "\n",
      "Q: W jakim zespole występowała Hanka w filmie „Żona dla Australijczyka”?\n",
      "A: Mazowsze\n",
      "Expected: Mazowsze\n",
      "Correct!\n",
      "\n",
      "Q: W którym państwie leży Bombaj?\n",
      "A: Indie.\n",
      "Expected: w Indiach\n",
      "Correct!\n",
      "\n",
      "Q: Który numer boczny nosi czołg Rudy z „Czterech pancernych”?\n",
      "A: 102\n",
      "Expected: 102\n",
      "Correct!\n",
      "\n",
      "Q: Co budował w Egipcie inżynier Tarkowski, ojciec Stasia?\n",
      "A: Budowle nie ma w treści.\n",
      "Expected: Kanał Sueski\n",
      "Incorrect!\n",
      "\n",
      "Q: Czy owoce niektórych kaktusów są jadalne?\n",
      "A: Tak.\n",
      "Expected: tak\n",
      "Correct!\n",
      "\n",
      "Q: Kwartet – to ilu wykonawców?\n",
      "A: czterech\n",
      "Expected: czterech\tczworo\t4\n",
      "Correct!\n",
      "\n",
      "Q: Którzy rzemieślnicy występują w ostatniej powieści Witkacego?\n",
      "A: Brak informacji o tym, że to ostatnia powieść Witkacego.\n",
      "Expected: szewcy\n",
      "Incorrect!\n",
      "\n",
      "Q: Jak nazywa się pierwiastek o symbolu Bq?\n",
      "A: Bekerel.\n",
      "Expected: becquerel\n",
      "Correct!\n",
      "\n",
      "Q: Hymnem którego kraju jest pieśń „Gwiaździsty sztandar”?\n",
      "A: Stanych Zjednoczonych Ameryki.\n",
      "Expected: Stanów Zjednoczonych\tUSA\n",
      "Correct!\n",
      "\n",
      "Q: Jak nazywał się sztywny kapelusz męski o zaokrąglonej główce i wąskim lekko uniesionym rondelku, modny w drugiej połowie XIX wieku?\n",
      "A: Kłobuk.\n",
      "Expected: melonik\n",
      "Incorrect!\n",
      "\n",
      "Q: Jak nazywa się wypukła albo wklęsła powierzchnia cieczy w pobliżu ścianek naczynia?\n",
      "A: Powierzchnia swobodna cieczy.\n",
      "Expected: menisk\n",
      "Incorrect!\n",
      "\n",
      "Q: Jak nazywał się fizyk, który w 1876 r. wynalazł telefon?\n",
      "A: Brak odpowiedzi w danych źródłach.\n",
      "Expected: Aleksander Graham Bell\n",
      "Incorrect!\n",
      "\n",
      "Q: Jak brzmi ulubione powiedzonko klucznika Horeszków Gerwazego?\n",
      "A: Horbatowicze to moje ulubione Horbatowicze!\n",
      "Expected: mopanku\n",
      "Incorrect!\n",
      "\n",
      "Q: Którego gazu jest najwięcej w powietrzu?\n",
      "A: Powietrze.\n",
      "Expected: azotu\n",
      "Incorrect!\n",
      "\n",
      "Q: Czy szalej jadowity występuje w Polsce?\n",
      "A: Tak.\n",
      "Expected: tak\n",
      "Correct!\n",
      "\n",
      "Q: Według przysłowia nos jest dla tabakiery czy tabakiera dla nosa?\n",
      "A: nie pasuje do pytania\n",
      "Expected: tabakiera dla nosa\n",
      "Incorrect!\n",
      "\n",
      "Correct: 11\n",
      "Accuracy: 55.00%\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def is_textual_match(pred, gold, threshold=0.5):\n",
    "    return levenshtein_distance(pred.lower(), gold.lower()) >= threshold\n",
    "\n",
    "def is_numerical_match(pred, gold):\n",
    "    pred_num = re.search(r\"\\d+\", pred)\n",
    "    gold_num = re.search(r\"\\d+\", gold)\n",
    "    if pred_num and gold_num:\n",
    "        return pred_num.group() == gold_num.group()\n",
    "    return False\n",
    "\n",
    "def evaluate(in_file, expected_file):\n",
    "    with open(in_file, 'r', encoding='utf-8') as file_in, open(expected_file, 'r', encoding='utf-8') as file_expected:\n",
    "        questions = [line.strip() for line in file_in]\n",
    "        gold_answers = [line.strip() for line in file_expected]\n",
    "    \n",
    "    total = 20\n",
    "    print(f\"Total questions: {total}\")\n",
    "    correct = 0\n",
    "\n",
    "    for index, (question, gold) in enumerate(zip(questions, gold_answers)):\n",
    "        if index >= total:\n",
    "            # My machine cannot handle more\n",
    "            break\n",
    "        paragraphs = retrieve_paragraphs(question, es, INDEX_NAME, size=8)\n",
    "        # best_answer, best_score = get_best_answer(question, paragraphs)\n",
    "        # print(f\"keywords: {get_keywords(question)}\")\n",
    "        # print(f\"Q: {question}\\nA: {best_answer}\\nExpected: {gold}\\n\\n\")\n",
    "        # print(f\"Score: {best_score}\\n\\n\")\n",
    "        # if best_answer is None:\n",
    "        #     continue\n",
    "\n",
    "        best_answer = get_best_answer_ollama(question, paragraphs)\n",
    "        print(f\"Q: {question}\\nA: {best_answer}\\nExpected: {gold}\")\n",
    "\n",
    "        if is_numerical_match(best_answer, gold) or is_textual_match(best_answer, gold):\n",
    "            print(\"Correct!\\n\")\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\"Incorrect!\\n\")\n",
    "\n",
    "    print(f\"Correct: {correct}\")\n",
    "    accuracy = float(correct) / float(total)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "DEV_0_IN = \"data/dev-0/in.tsv\"\n",
    "DEV_0_EXPECTED = \"data/dev-0/expected.tsv\"\n",
    "evaluate(DEV_0_IN, DEV_0_EXPECTED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
