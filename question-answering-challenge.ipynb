{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Wikipedia dump for offline retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in https://en.wikipedia.org/wiki/Wikipedia:Database_download, we download the Polish Wikipedia dump from https://dumps.wikimedia.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plwiki-latest-pages-articles.xml.bz2:   1%|          | 19.7M/2.35G [00:22<4:24:50, 158kB/s]"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://dumps.wikimedia.org/plwiki/latest/plwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    file_name = 'plwiki-latest-pages-articles.xml.bz2'\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = 1024  # 1 KB\n",
    "    with open(file_name, 'wb') as file, tqdm(\n",
    "        desc=file_name,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    print(f'Wikipedia dump downloaded successfully: {file_name}')\n",
    "else:\n",
    "    print(f\"Failed to download the Wikipedia dump. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from the Wikipedia dump using WikiExtractor\n",
    "\n",
    "Run the following command to extract articles from the dump into an `extracted/` folder:\n",
    "```bash\n",
    "wikiextractor --json plwiki-latest-pages-articles.xml.bz2 -o extracted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the extracted data and indexing it into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv(\"elastic-start-local/.env\")\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", api_key=os.getenv(\"ES_LOCAL_API_KEY\"))\n",
    "\n",
    "INDEX_NAME = 'wiki_index'\n",
    "INDEX_BODY = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"content\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    \n",
    "if not es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_BODY)\n",
    "    \n",
    "def index_articles(dump_dir, es, index_name):\n",
    "    for root, _, files in os.walk(dump_dir):\n",
    "        for file in files:\n",
    "            print(f'Indexing file: {file}')\n",
    "            if file.startswith('wiki_'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        article = json.loads(line)\n",
    "                        title = article.get('title', '')\n",
    "                        content = article.get('text', '')\n",
    "                        \n",
    "                        es.index(index=index_name, body={\n",
    "                            \"title\": title,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "\n",
    "index_articles('wiki-dump', es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and answer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at sdadas/polish-gpt2-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\n",
      "A: Playboy\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"question-answering\", model=\"sdadas/polish-gpt2-large\")\n",
    "\n",
    "def retrieve_context(question, es, index_name, num_results=3):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": question,\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"operator\": \"and\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": num_results\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    \n",
    "    context = \" \".join(hit['_source']['content'] for hit in response['hits']['hits'])\n",
    "    return context\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    result = model(question=question, context=context)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "def quiz_answer_system(question, es, index_name):\n",
    "    context = retrieve_context(question, es, index_name)\n",
    "    return generate_answer(question, context)\n",
    "\n",
    "question = \"Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\"\n",
    "answer = quiz_answer_system(question, es, INDEX_NAME)\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
