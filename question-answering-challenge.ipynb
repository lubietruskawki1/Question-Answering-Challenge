{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Wikipedia dump for offline retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in https://en.wikipedia.org/wiki/Wikipedia:Database_download, we download the Polish Wikipedia dump from https://dumps.wikimedia.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plwiki-latest-pages-articles.xml.bz2:   1%|          | 19.7M/2.35G [00:22<4:24:50, 158kB/s]"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://dumps.wikimedia.org/plwiki/latest/plwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    file_name = 'plwiki-latest-pages-articles.xml.bz2'\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = 1024  # 1 KB\n",
    "    with open(file_name, 'wb') as file, tqdm(\n",
    "        desc=file_name,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    print(f'Wikipedia dump downloaded successfully: {file_name}')\n",
    "else:\n",
    "    print(f\"Failed to download the Wikipedia dump. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from the Wikipedia dump using WikiExtractor\n",
    "\n",
    "Run the following command to extract articles from the dump into an `extracted/` folder:\n",
    "```bash\n",
    "wikiextractor --json plwiki-latest-pages-articles.xml.bz2 -o extracted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the extracted data and indexing it into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv(\"elastic-start-local/.env\")\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", api_key=os.getenv(\"ES_LOCAL_API_KEY\"))\n",
    "\n",
    "INDEX_NAME = 'wiki_index'\n",
    "INDEX_BODY = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"content\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    \n",
    "if not es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_BODY)\n",
    "    \n",
    "def index_articles(dump_dir, es, index_name):\n",
    "    for root, _, files in os.walk(dump_dir):\n",
    "        for file in files:\n",
    "            print(f'Indexing file: {file}')\n",
    "            if file.startswith('wiki_'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        article = json.loads(line)\n",
    "                        title = article.get('title', '')\n",
    "                        content = article.get('text', '')\n",
    "                        \n",
    "                        es.index(index=index_name, body={\n",
    "                            \"title\": title,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "\n",
    "index_articles('wiki-dump', es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and answer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at sdadas/polish-gpt2-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"question-answering\", model=\"sdadas/polish-gpt2-large\")\n",
    "\n",
    "def retrieve_context(question, es, index_name, num_results=3):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": question,\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"operator\": \"and\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": num_results\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    \n",
    "    context = \" \".join(hit['_source']['content'] for hit in response['hits']['hits'])\n",
    "    return context\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    result = model(question=question, context=context)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "def quiz_answer_system(question, es, index_name):\n",
    "    context = retrieve_context(question, es, index_name)\n",
    "    return generate_answer(question, context) if context else \"No answer found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\n",
      "A:  w poprzednich częściach, a zamiast tego skupić się na realistycznych proporcjach\n"
     ]
    }
   ],
   "source": [
    "question = \"Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\"\n",
    "answer = quiz_answer_system(question, es, INDEX_NAME)\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Jak nazywa się pierwsza litera alfabetu greckiego?\n",
      "A:  wielu alfabetom\n",
      "Expected: alfa\n",
      "\n",
      "\n",
      "Q: Jak nazywa się dowolny odcinek łączący dwa punkty okręgu?\n",
      "A: No answer found\n",
      "Expected: cięciwa\n",
      "\n",
      "\n",
      "Q: W którym państwie rozpoczyna się akcja powieści „W pustyni i w puszczy”?\n",
      "A: No answer found\n",
      "Expected: w Egipcie\n",
      "\n",
      "\n",
      "Q: Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\n",
      "A: No answer found\n",
      "Expected: tak\n",
      "\n",
      "\n",
      "Q: W jakim zespole występowała Hanka w filmie „Żona dla Australijczyka”?\n",
      "A: No answer found\n",
      "Expected: Mazowsze\n",
      "\n",
      "\n",
      "Q: W którym państwie leży Bombaj?\n",
      "A: No answer found\n",
      "Expected: w Indiach\n",
      "\n",
      "\n",
      "Q: Który numer boczny nosi czołg Rudy z „Czterech pancernych”?\n",
      "A: No answer found\n",
      "Expected: 102\n",
      "\n",
      "\n",
      "Q: Co budował w Egipcie inżynier Tarkowski, ojciec Stasia?\n",
      "A: No answer found\n",
      "Expected: Kanał Sueski\n",
      "\n",
      "\n",
      "Q: Czy owoce niektórych kaktusów są jadalne?\n",
      "A: No answer found\n",
      "Expected: tak\n",
      "\n",
      "\n",
      "Q: Kwartet – to ilu wykonawców?\n",
      "A: No answer found\n",
      "Expected: czterech\tczworo\t4\n",
      "\n",
      "\n",
      "Accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def is_textual_match(pred, gold, threshold=0.5):\n",
    "    return levenshtein_distance(pred.lower(), gold.lower()) >= threshold\n",
    "\n",
    "def is_numerical_match(pred, gold):\n",
    "    pred_num = re.search(r\"\\d+\", pred)\n",
    "    gold_num = re.search(r\"\\d+\", gold)\n",
    "    if pred_num and gold_num:\n",
    "        return pred_num.group() == gold_num.group()\n",
    "    return False\n",
    "\n",
    "def evaluate(in_file, expected_file):\n",
    "    with open(in_file, 'r', encoding='utf-8') as file_in, open(expected_file, 'r', encoding='utf-8') as file_expected:\n",
    "        questions = [line.strip() for line in file_in]\n",
    "        gold_answers = [line.strip() for line in file_expected]\n",
    "    \n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "\n",
    "    for index, (question, gold) in enumerate(zip(questions, gold_answers)):\n",
    "        if index >= 10:\n",
    "            # My machine cannot handle more\n",
    "            break\n",
    "        pred = quiz_answer_system(question, es, INDEX_NAME)\n",
    "        print(f\"Q: {question}\\nA: {pred}\\nExpected: {gold}\\n\\n\")\n",
    "        if is_numerical_match(pred, gold) or is_textual_match(pred, gold):\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "DEV_0_IN = \"data/dev-0/in.tsv\"\n",
    "DEV_0_EXPECTED = \"data/dev-0/expected.tsv\"\n",
    "evaluate(DEV_0_IN, DEV_0_EXPECTED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
