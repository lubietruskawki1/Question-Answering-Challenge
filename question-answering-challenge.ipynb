{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Wikipedia dump for offline retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in https://en.wikipedia.org/wiki/Wikipedia:Database_download, we download the Polish Wikipedia dump from https://dumps.wikimedia.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plwiki-latest-pages-articles.xml.bz2: 100%|██████████| 2.35G/2.35G [2:06:48<00:00, 332kB/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dump downloaded successfully: plwiki-latest-pages-articles.xml.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://dumps.wikimedia.org/plwiki/latest/plwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    file_name = 'plwiki-latest-pages-articles.xml.bz2'\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = 1024  # 1 KB\n",
    "    with open(file_name, 'wb') as file, tqdm(\n",
    "        desc=file_name,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "    print(f'Wikipedia dump downloaded successfully: {file_name}')\n",
    "else:\n",
    "    print(f\"Failed to download the Wikipedia dump. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from the Wikipedia dump using WikiExtractor\n",
    "\n",
    "Run the following command to extract articles from the dump into an `extracted/` folder:\n",
    "```bash\n",
    "wikiextractor --json plwiki-latest-pages-articles.xml.bz2 -o extracted # --processes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the extracted data and indexing it into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "# -------------------------------------------\n",
    "# KONFIGURACJA\n",
    "# -------------------------------------------\n",
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "# Mappings (uwaga: \"paragraph_number\" to \"integer\", nie \"number\")\n",
    "INDEX_BODY = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"paragraph_number\": {\"type\": \"integer\"},\n",
    "            \"content\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adres Elasticsearch (zakładamy, że działa na http://localhost:9200)\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# UTWORZENIE INDEKSU (jeśli nie istnieje)\n",
    "# -------------------------------------------\n",
    "if not es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_BODY)\n",
    "    # Wyłączamy odświeżanie i replikę na czas indeksowania (przyspieszy to import)\n",
    "    es.indices.put_settings(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"index\": {\n",
    "                \"refresh_interval\": \"-1\",\n",
    "                \"number_of_replicas\": 0\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created index '{INDEX_NAME}' with temporary settings (no refresh, 0 replicas).\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# DEFINICJA GENERATORA DOKUMENTÓW\n",
    "# -------------------------------------------\n",
    "def generate_actions(dump_dir, index_name):\n",
    "    \"\"\"\n",
    "    Generator dla parallel_bulk. Dla każdego pliku wiki_... wyciąga artykuły,\n",
    "    dzieli je na paragrafy i yielduje dokumenty do zindeksowania.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(dump_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('wiki_'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        article = json.loads(line)\n",
    "                        title = article.get('title', '')\n",
    "                        content = article.get('text', '')\n",
    "\n",
    "                        paragraphs = content.split(\"\\n\\n\")\n",
    "                        for i, paragraph in enumerate(paragraphs):\n",
    "                            paragraph = paragraph.strip()\n",
    "                            if paragraph:\n",
    "                                yield {\n",
    "                                    \"_index\": index_name,\n",
    "                                    \"_source\": {\n",
    "                                        \"title\": title,\n",
    "                                        \"paragraph_number\": i,\n",
    "                                        \"content\": paragraph\n",
    "                                    }\n",
    "                                }\n",
    "\n",
    "# -------------------------------------------\n",
    "# WYWOŁANIE parallel_bulk DO MASOWEGO INDEKSOWANIA\n",
    "# -------------------------------------------\n",
    "actions = generate_actions(\"extracted\", INDEX_NAME)\n",
    "\n",
    "successes = 0\n",
    "for ok, resp in parallel_bulk(\n",
    "    client=es,\n",
    "    actions=actions,\n",
    "    thread_count=4,      # liczba wątków\n",
    "    chunk_size=500,      # wielkość jednej paczki dokumentów\n",
    "    max_chunk_bytes=10 * 1024 * 1024  # ~10 MB na paczkę\n",
    "):\n",
    "    if not ok:\n",
    "        logging.error(f\"Error indexing chunk: {resp}\")\n",
    "    else:\n",
    "        successes += 1\n",
    "\n",
    "print(f\"Successfully processed {successes} chunks of data.\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# PRZYWRACANIE NORMALNYCH USTAWIEŃ\n",
    "# -------------------------------------------\n",
    "es.indices.put_settings(\n",
    "    index=INDEX_NAME,\n",
    "    body={\n",
    "        \"index\": {\n",
    "            \"refresh_interval\": \"1s\",\n",
    "            \"number_of_replicas\": 1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Indexing complete. Restored normal settings for '{INDEX_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve keywords from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "def get_keywords(question):\n",
    "    doc = nlp(question)\n",
    "    keywords = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"VERB\", \"NUM\", \"ADJ\"] and not token.is_stop]\n",
    "    return \" \".join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and answer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "# Inicjalizacja Elasticsearch i pipeline QA\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"radlab/polish-qa-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "# 2. Funkcja pobierająca akapity z ES\n",
    "def retrieve_paragraphs(question, es_client, index_name, size=10):\n",
    "    \"\"\"\n",
    "    Wyszukuje w indeksie 'index_name' paragrafy z fields: ['title', 'content'] pasujące do 'question'.\n",
    "    Zwraca listę (max 'size') tekstów - każdy to pojedynczy paragraf.\n",
    "    \"\"\"\n",
    "    print(get_keywords(question))\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": question,\n",
    "                \"fields\": [\"title\", \"content\"],\n",
    "                \"operator\": \"or\",\n",
    "                \"minimum_should_match\": \"60%\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=query)\n",
    "\n",
    "    paragraphs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        # Zakładamy, że w _source mamy klucz 'content' z tekstem paragrafu\n",
    "        paragraphs.append(hit[\"_source\"][\"content\"])\n",
    "    return paragraphs\n",
    "\n",
    "# 3. Funkcja, która spośród pobranych paragrafów wybiera najlepszą odpowiedź\n",
    "def get_best_answer(question, paragraphs):\n",
    "    \"\"\"\n",
    "    Dla każdego paragrafu wywołuje pipeline QA. \n",
    "    Zwraca (best_answer, best_score).\n",
    "    \"\"\"\n",
    "    best_answer = None\n",
    "    best_score = float(\"-inf\")\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.strip():\n",
    "            result = qa_pipeline(question=question, context=paragraph)\n",
    "            if result[\"score\"] > best_score:\n",
    "                best_score = result[\"score\"]\n",
    "                best_answer = result[\"answer\"]\n",
    "\n",
    "    return best_answer, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Przykładowe wywołanie\n",
    "\n",
    "def answer_question(question, es, index_name):\n",
    "    paragraphs = retrieve_paragraphs(question, es, index_name, size=10)\n",
    "    best_answer, best_score = get_best_answer(question, paragraphs)\n",
    "    \n",
    "    print(f\"Pytanie: {question}\")\n",
    "    print(f\"Najlepsza odpowiedź: {best_answer}\")\n",
    "    print(f\"Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stworzył Tomb Raider\n",
      "Pytanie: Kto stworzył Tomb Raider?\n",
      "Najlepsza odpowiedź:  Toby Gard i Paul Howard Douglasa,\n",
      "Score: 0.9921822547912598\n"
     ]
    }
   ],
   "source": [
    "question = \"Kto stworzył Tomb Raider?\"\n",
    "answer_question(question, es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nazywa bohaterka gier komputerowych serii Tomb Raider\n",
      "Pytanie: Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\n",
      "Najlepsza odpowiedź:  Lary Croft\n",
      "Score: 0.9942718744277954\n",
      "państwach starożytnych powoływani posłowie poselstwa\n",
      "Pytanie: Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\n",
      "Najlepsza odpowiedź:  byli powoływani do służby wojskowej\n",
      "Score: 0.10543614625930786\n"
     ]
    }
   ],
   "source": [
    "question = \"Jak nazywa się bohaterka gier komputerowych z serii Tomb Raider?\"\n",
    "answer_question(question, es, INDEX_NAME)\n",
    "\n",
    "question = \"Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\"\n",
    "paragraphs = retrieve_paragraphs(question, es, INDEX_NAME, size=10)\n",
    "question_tuned = \"Czy w starożytności powoływani byli posłowie i poselstwa? Odpowiedz jednym słowem, tak lub nie.\"\n",
    "best_answer, best_score = get_best_answer(question_tuned, paragraphs)\n",
    "\n",
    "print(f\"Pytanie: {question}\")\n",
    "print(f\"Najlepsza odpowiedź: {best_answer}\")\n",
    "print(f\"Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pełnych tygodni rok kalendarzowy\n",
      "Pytanie: Ile pełnych tygodni ma rok kalendarzowy?\n",
      "Najlepsza odpowiedź:  dziesięciu\n",
      "Score: 0.8816742300987244\n"
     ]
    }
   ],
   "source": [
    "question = \"Ile pełnych tygodni ma rok kalendarzowy?\"\n",
    "answer_question(question, es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nazywa pierwsza litera alfabetu greckiego\n",
      "Q: Jak nazywa się pierwsza litera alfabetu greckiego?\n",
      "A: Alfa\n",
      "Expected: alfa\n",
      "\n",
      "\n",
      "Score: 0.953810453414917\n",
      "\n",
      "\n",
      "nazywa dowolny odcinek łączący punkty okręgu\n",
      "Q: Jak nazywa się dowolny odcinek łączący dwa punkty okręgu?\n",
      "A:  okręgu.\n",
      "Cięciwą\n",
      "Expected: cięciwa\n",
      "\n",
      "\n",
      "Score: 0.990360677242279\n",
      "\n",
      "\n",
      "państwie rozpoczyna akcja powieści pustyni puszczy\n",
      "Q: W którym państwie rozpoczyna się akcja powieści „W pustyni i w puszczy”?\n",
      "A:  egipskiego\n",
      "Expected: w Egipcie\n",
      "\n",
      "\n",
      "Score: 0.8773928880691528\n",
      "\n",
      "\n",
      "państwach starożytnych powoływani posłowie poselstwa\n",
      "Q: Czy w państwach starożytnych powoływani byli posłowie i poselstwa?\n",
      "A:  System taki panował głównie w państwach starożytnych\n",
      "Expected: tak\n",
      "\n",
      "\n",
      "Score: 0.4257482886314392\n",
      "\n",
      "\n",
      "zespole występowała Hanka filmie Żona Australijczyka\n",
      "Q: W jakim zespole występowała Hanka w filmie „Żona dla Australijczyka”?\n",
      "A:  Meteory\n",
      "Expected: Mazowsze\n",
      "\n",
      "\n",
      "Score: 0.9639838933944702\n",
      "\n",
      "\n",
      "państwie leży Bombaj\n",
      "Q: W którym państwie leży Bombaj?\n",
      "A:  indyjski\n",
      "Expected: w Indiach\n",
      "\n",
      "\n",
      "Score: 0.9778725504875183\n",
      "\n",
      "\n",
      "numer boczny nosi czołg Rudy Czterech pancernych\n",
      "Q: Który numer boczny nosi czołg Rudy z „Czterech pancernych”?\n",
      "A:  102,\n",
      "Expected: 102\n",
      "\n",
      "\n",
      "Score: 0.9709969162940979\n",
      "\n",
      "\n",
      "budował Egipcie inżynier Tarkowski ojciec Stasia\n",
      "Q: Co budował w Egipcie inżynier Tarkowski, ojciec Stasia?\n",
      "A:  świątyni słońca\n",
      "Expected: Kanał Sueski\n",
      "\n",
      "\n",
      "Score: 0.9644864201545715\n",
      "\n",
      "\n",
      "owoce kaktusów jadalne\n",
      "Q: Czy owoce niektórych kaktusów są jadalne?\n",
      "A:  Owoce są jadalne.\n",
      "Systematyka.\n",
      "Jeden\n",
      "Expected: tak\n",
      "\n",
      "\n",
      "Score: 0.994006872177124\n",
      "\n",
      "\n",
      "Kwartet wykonawców\n",
      "Q: Kwartet – to ilu wykonawców?\n",
      "A:  kwartet.\n",
      "Expected: czterech\tczworo\t4\n",
      "\n",
      "\n",
      "Score: 0.9396554827690125\n",
      "\n",
      "\n",
      "Accuracy: 0.50%\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def is_textual_match(pred, gold, threshold=0.5):\n",
    "    return levenshtein_distance(pred.lower(), gold.lower()) >= threshold\n",
    "\n",
    "def is_numerical_match(pred, gold):\n",
    "    pred_num = re.search(r\"\\d+\", pred)\n",
    "    gold_num = re.search(r\"\\d+\", gold)\n",
    "    if pred_num and gold_num:\n",
    "        return pred_num.group() == gold_num.group()\n",
    "    return False\n",
    "\n",
    "def evaluate(in_file, expected_file):\n",
    "    with open(in_file, 'r', encoding='utf-8') as file_in, open(expected_file, 'r', encoding='utf-8') as file_expected:\n",
    "        questions = [line.strip() for line in file_in]\n",
    "        gold_answers = [line.strip() for line in file_expected]\n",
    "    \n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "\n",
    "    for index, (question, gold) in enumerate(zip(questions, gold_answers)):\n",
    "        if index >= 10:\n",
    "            # My machine cannot handle more\n",
    "            break\n",
    "        paragraphs = retrieve_paragraphs(question, es, INDEX_NAME, size=10)\n",
    "        best_answer, best_score = get_best_answer(question, paragraphs)\n",
    "        print(f\"Q: {question}\\nA: {best_answer}\\nExpected: {gold}\\n\\n\")\n",
    "        print(f\"Score: {best_score}\\n\\n\")\n",
    "        if is_numerical_match(best_answer, gold) or is_textual_match(best_answer, gold):\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "DEV_0_IN = \"data/dev-0/in.tsv\"\n",
    "DEV_0_EXPECTED = \"data/dev-0/expected.tsv\"\n",
    "evaluate(DEV_0_IN, DEV_0_EXPECTED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
